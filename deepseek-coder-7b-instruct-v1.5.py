# -*- coding: utf-8 -*-
"""
Created on Tue Aug  5 15:02:23 2025

@author: darek
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Sprawdzenie GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"UÅ¼ywane urzÄ…dzenie: {device}")

# Nazwa modelu z HuggingFace
model_id = "deepseek-ai/deepseek-coder-7b-instruct-v1.5"

# Wczytanie tokenizer'a
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Wczytanie modelu z quantization (jeÅ›li RAM GPU ograniczony)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,         # uÅ¼yj float16 do zmniejszenia VRAM
    device_map="auto",                 # automatyczne przypisanie do GPU
    low_cpu_mem_usage=True,           # oszczÄ™dnoÅ›Ä‡ RAM
)

# PrzykÅ‚adowy prompt
prompt = '''<|system|>
JesteÅ› wykwalifikowanym pomocnikiem do analizy indeksÃ³w ksiÄ…Å¼ek i danych bibliograficznych. Przetwarzasz niestandardowy tekst z indeksu i konwertujesz go na ustandaryzowanÄ… tabelÄ™ danych w formacie CSV.

<|user|>
PrzetwÃ³rz poniÅ¼szy surowy tekst indeksowy na tabelÄ™ danych w formacie:
Nazwa;Opis;Strony

KaÅ¼da linia powinna zawieraÄ‡:
- nazwÄ™ osoby, miejsca lub rzeczy (np. "Acosta, Iosephus de, S.I.")
- krÃ³tki opis (np. "missionarius per regionem de La Plata")
- numer strony lub strony (np. "629, 709")

Dane wejÅ›ciowe:

Abbreviationes in hoc volumine quid significent , 53 .
Acapulco , portus , 610 27.
Acevedo, Ignatius de, S. I., Borgia dat ei instructionem missionariam , 120, 122 , 124 11.
Acllas , 21s .
Acosta,Iosephus de,S.I.,vita 299, manifestat Borgiae suum desiderium Indias adeundi , 300-302 ; pro cleri educatione , 302 ; P. Ludovicum GuzmÃ¡n ad missiones praesentat , 303 ; revelat P. Nadal voluntatem suam ad missiones indicas et varia de se ipso , 300 , 301 ; conficit litteras annuas , 300 ; pro Roma vel Burgos destinatus , 322 ; Peruae missionarius renuntiatus , 389 , 390 , 391 , 371 ; nuntiat Borgiae praeludia itineris , 439-442 ; quaerit de P. Fonseca , 442 ; proponit ad Sacros Ordines F. D. MartÃ­nez, 442 ; Hispali versatur et SanlÃºcar , 440 ; in insula S. Ioannis et S. Dominici , 443 ; eius iter a Sacchini narratur , 47s ; Limam attigit, 36, 505 ; confessarius in collegio limensi et magister novitiorum , 505 ; concionator Limae , 703 ; missionarius per regionem de La Plata , 629 , 709 ; eius missio per Peruam,629,706 ; ad PotosÃ­,709 ; quaestiones morales cum eo conferendae , 632 ; provincialis designatus Peruae, 37 ; eius actio in Congregationibus provincialibus , 38 ; scriptor, 321 ; informationes de eo, 301 ; laudatur , 505 , 507 , 509 , 589.

ZwrÃ³Ä‡ wynik w formacie CSV (Å›redniki), bez zbÄ™dnych znakÃ³w lub komentarzy. JeÅ›li sÄ… wielokrotne wpisy dla tej samej osoby, utwÃ³rz osobne linie z odpowiednim opisem.
'''
inputs = tokenizer(prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=3000)
# outputs = model.generate(
#     **inputs,
#     max_new_tokens=100,
#     do_sample=True,
#     temperature=0.7,
#     top_p=0.9,
#     top_k=50,
#     repetition_penalty=1.1,
#     pad_token_id=tokenizer.eos_token_id  # Dodajemy jawnie, Å¼eby uniknÄ…Ä‡ warningu
# )
# Dekodowanie odpowiedzi
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nWygenerowana odpowiedÅº:\n")
print(response)

#%%
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import os
# -------------------- KONFIGURACJA ---------------------
MODEL_ID   = "meta-llama/Llama-3.1-8B-Instruct" #"tiiuae/Falcon3-10B-Instruct"
USE_INT8   = True
TEMPERATURE = 0.0
MAX_TOK    = 1300
#OVERLAP    = 150
from huggingface_hub import login
token = os.getenv("HF_TOKEN")
# -------------------- TOKENIZER ------------------------
tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
if tok.pad_token is None:
    tok.pad_token = tok.eos_token
# -------------------- MODEL ----------------------------
if USE_INT8:
    quant_config = BitsAndBytesConfig(load_in_8bit=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quant_config,
        device_map="auto"
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

# -------------------- USTAWIENIA GENERACJI ----------------------------
model.generation_config.update(
    temperature=TEMPERATURE,
    do_sample=False,
    pad_token_id=tok.eos_token_id
)

# -------------------- PROMPT ----------------------------
prompt = '''You are a highly skilled assistant for processing book indexes and bibliographic data. Your task is to transform irregular index text into a strictly structured, semicolon-delimited CSV format.

Requirements:
- Use â€œ;â€ as the field delimiter.
- Do not quote fieldsâ€”even if they contain commas.
- Output columns in this order: Name;Description;Pages
- Each Pages field may contain one or multiple page numbers or ranges, separated by commas.

Here is an example of the desired output format:

Name;Description;Pages
Abbreviationes;Quid significant in hoc volumine;53
Acapulco;Portus;610
Acevedo, Ignatius de, S.I.;Borgia dat ei instructionem missionariam;120, 122, 124
Acosta, Iosephus de, S.I.;Missionarius per regionem de La Plata;629, 709
Agnus Dei;Ab eo recipiuntur;140
Aguado, Petrus de, O.F.M.;Scriptor;130

Now transform the following raw index text into that CSV form, one entry per line:

Abbreviationes in hoc volumine quid significent, 53.
Acapulco, portus, 610.
Acevedo, Ignatius de, S. I., Borgia dat ei instructionem missionariam, 120, 122, 124.
Acllas, 21s.
Acosta, Iosephus de, S.I., vita, 299; manifestat Borgiae suum desiderium Indias adeundi, 300â€“302; pro cleri educatione, 302; P. Ludovicum GuzmÃ¡n ad missiones praesentat, 303; revelat P. Nadal voluntatem suam ad missiones indicas et varia de se ipso, 300, 301; conficit litteras annuas, Roma vel Burgos destinatus, 322; Peruae missionarius renuntiatus, 389, 390, 391, 371; nuntiat Borgiae praeludia itineris, 439â€“442; quaerit de P. Fonseca, 442; proponit ad Sacros Ordines F. D. MartÃ­nez, 442; Hispali versatur et SanlÃºcar, 440; in insula S. Ioannis et S. Dominici, 443; eius iter a Sacchini narratur, 47s; Limam attigit, 36, 505; confessarius in collegio limensi et magister novitiorum, 505; concionator Limae, 703; missionarius per regionem de La Plata, 629, 709; eius missio per Peruam, 629, 706; ad PotosÃ­, 709; quaestiones morales cum eo conferendae, 632; provincialis designatus Peruae, 37; eius actio in Congregationibus provincialibus, 38; scriptor, 321; informationes de eo, 301; laudatur, 505, 507, 509, 589.

'''

# -------------------- TOKENIZACJA I GENERACJA ----------------------------
inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=MAX_TOK).to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=MAX_TOK  # wykorzystanie staÅ‚ej
)

# -------------------- WYNIK ----------------------------
response = tok.decode(outputs[0], skip_special_tokens=True)
print("\nWygenerowana odpowiedÅº:\n")
print(response)

#%%

import re
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# -------------------- KONFIGURACJA ---------------------
MODEL_ID    = "tiiuae/Falcon3-10B-Instruct"
USE_INT8    = True
TEMPERATURE = 0.0
MAX_TOK     = 1300
PREVIEW_CHUNKS = 3

# -------------------- WCZYTAJ MODEL I TOKENIZER ---------------------
tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
if USE_INT8:
    quant_config = BitsAndBytesConfig(load_in_8bit=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=quant_config,
        device_map="auto"
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
model.generation_config.update(
    temperature=TEMPERATURE,
    do_sample=False,
    pad_token_id=tok.eos_token_id
)

# -------------------- PROMPT (staÅ‚a czÄ™Å›Ä‡) ---------------------
BASE_PROMPT = """You are a highly skilled assistant for processing book indexes and bibliographic data. Your task is to transform irregular index text into a strictly structured, semicolon-delimited CSV format.

Requirements:
- Use â€œ;â€ as the field delimiter.
- Do not quote fieldsâ€”even if they contain commas.
- Output columns in this order: Name;Description;Pages
- Each Pages field may contain one or multiple page numbers or ranges, separated by commas.

Here is an example of the desired output format:

Name;Description;Pages
Abbreviationes;Quid significant in hoc volumine;53
Acapulco;Portus;610
Acevedo, Ignatius de, S.I.;Borgia dat ei instructionem missionariam;120, 122, 124
Acosta, Iosephus de, S.I.;Missionarius per regionem de La Plata;629, 709
Agnus Dei;Ab eo recipiuntur;140
Aguado, Petrus de, O.F.M.;Scriptor;130

Now transform the following raw index text into that CSV form, one entry per line:
"""

# -------------------- FUNKCJE ---------------------
def parse_csv_response(text):
    entries = []
    for line in text.splitlines():
        if not line.strip() or line.startswith("Name;"):
            continue
        parts = line.strip().split(";")
        if len(parts) >= 3:
            entries.append({
                "name": parts[0].strip(),
                "description": parts[1].strip(),
                "pages": parts[2].strip()
            })
    return entries

def chunk_text(entries, tokenizer, max_token_len):
    chunks = []
    current_chunk = ""
    for entry in entries:
        token_len = len(tokenizer(entry, return_tensors="pt").input_ids[0])
        current_len = len(tokenizer(current_chunk, return_tensors="pt").input_ids[0]) if current_chunk else 0
        if current_len + token_len > max_token_len:
            chunks.append(current_chunk.strip())
            current_chunk = entry
        else:
            current_chunk += " " + entry
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    return chunks

# -------------------- WCZYTAJ I PRZYGOTUJ DANE ---------------------
with open("C:/pdf_llm_do_roboty/indkes.txt", encoding="utf-8") as f:
    raw_text = f.read()

# UproÅ›Ä‡ i wyczyÅ›Ä‡
cleaned_text = re.sub(r"\s+", " ", raw_text)
entries = re.split(r"(?=(?:[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»][^,]{1,80},))", cleaned_text)
entries = [e.strip().strip(".") for e in entries if len(e.strip()) > 10]

# Podziel na chunk'i
chunks = chunk_text(entries, tok, MAX_TOK - 400)
print(f"\nðŸ”¹ Znaleziono {len(entries)} jednostek, podzielono na {len(chunks)} chunkÃ³w\n")

# PokaÅ¼ kilka chunkÃ³w do kontroli
for i in range(min(PREVIEW_CHUNKS, len(chunks))):
    print(f"ðŸ§© CHUNK {i+1}:\n{'-'*40}\n{chunks[i][:700]}...\n{'-'*40}\n")

# -------------------- GENERUJEMY ---------------------
all_data = []
for idx, chunk in enumerate(chunks):
    prompt = BASE_PROMPT + chunk
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=MAX_TOK).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=MAX_TOK)
    decoded = tok.decode(outputs[0], skip_special_tokens=True)

    # Parsuj i dodaj
    parsed_entries = parse_csv_response(decoded)
    all_data.extend(parsed_entries)
    print(f"âœ… Wygenerowano chunk {idx+1}/{len(chunks)} ({len(parsed_entries)} rekordÃ³w)")

# -------------------- ZAPIS DO JSON ---------------------
with open("index_parsed.json", "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=2)

print(f"\nðŸŽ‰ Gotowe! Zapisano {len(all_data)} rekordÃ³w do pliku index_parsed.json.")
