import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "speakleash/Bielik-11B-v2.3-Instruct"

# Åadujemy tokenizer i model Bielika w float16 na GPU:
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype=torch.float16,
#     trust_remote_code=True
# ).cuda()

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_8bit=True,
    trust_remote_code=True
)
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
    #device=0  # GPU
)

def ask_bielik(prompt: str, max_new_tokens=1000) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.1,   # moÅ¼esz zmieniÄ‡
        top_p=0.9,
        repetition_penalty=1.2
    )
    return output[0]["generated_text"]

prompt = "opowiedz mi o mickiewiczu adamie"
response = ask_bielik(prompt)
print(response)
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline

# Nazwa modelu
model_name = "CYFRAGOVPL/PLLuM-12B-nc-instruct"

# Konfiguracja kwantyzacji
quant_config = BitsAndBytesConfig(
    load_in_8bit=True,                         # WÅ‚Ä…cz kwantyzacjÄ™ 8-bitowÄ…
    llm_int8_enable_fp32_cpu_offload=True      # Offload czÄ™Å›ci modelu na CPU
)

# Wczytanie tokenizera
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Wczytanie modelu z konfiguracjÄ… kwantyzacji
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",                         # Automatyczne przypisanie GPU/CPU
    quantization_config=quant_config,         # Nowa metoda dla 8-bit
    trust_remote_code=True
)

# Pipeline do generowania tekstu
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
)

# Funkcja do zadawania pytaÅ„
def ask_pllum(prompt: str, max_new_tokens=500) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        temperature=0.0,
        top_p=0.7,
        repetition_penalty=1.2
    )
    return output[0]["generated_text"]

# PrzykÅ‚ad uÅ¼ycia
prompt = "WyjaÅ›nij zasadÄ™ dziaÅ‚ania silnika spalinowego."
response = ask_pllum(prompt)
print(response)

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline

# Nazwa modelu
model_name = "CYFRAGOVPL/Llama-PLLuM-8B-instruct"

# Konfiguracja kwantyzacji
quant_config = BitsAndBytesConfig(
    load_in_8bit=True,                         # WÅ‚Ä…cz kwantyzacjÄ™ 8-bitowÄ…
    llm_int8_enable_fp32_cpu_offload=True      # Offload czÄ™Å›ci modelu na CPU
)

# Wczytanie tokenizera
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Wczytanie modelu z konfiguracjÄ… kwantyzacji
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",                         # Automatyczne przypisanie GPU/CPU
    quantization_config=quant_config,         # Nowa metoda dla 8-bit
    trust_remote_code=True
)
model.eval()
# Pipeline do generowania tekstu
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
)

# Funkcja do zadawania pytaÅ„
def ask_llama_pllum(prompt: str, max_new_tokens=1000) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.8,
        top_p=0.7,
        repetition_penalty=1.2
    )
    return output[0]["generated_text"]

# PrzykÅ‚ad uÅ¼ycia
prompt = "opowiedz mi o mickiewiczu adamie"
response = ask_llama_pllum(prompt)
print(response)

import datetime

# Oblicz bieÅ¼Ä…cy rok
current_year = datetime.datetime.now().year

# Dynamically insert the current year into the prompt:
interpret_prompt = f"""JesteÅ› asystentem bibliotecznym.

Masz wyÅ‚Ä…cznie ustaliÄ‡, czy w pytaniu uÅ¼ytkownika wystÄ™puje:
- tytuÅ‚,
- autor,
- tematyka,
- zakres dat (year range),
- pojedyncza data (year),
- miejsce wydania (place),
- wydawca (publisher),
- jÄ™zyk (language),
- typ dokumentu (document_type),
lub jakiekolwiek kombinacje powyÅ¼szych.

Wynik przedstaw **wyÅ‚Ä…cznie** w formacie:
[author: "nazwisko"], [subject: "temat"], [title: "tytuÅ‚"], [year: "RRRR"], [year_range: "RRRR-RRRR"], [place: "miejsce wydania"], [publisher: "wydawca"], [language: "jÄ™zyk"], [document_type: "typ dokumentu"]

JeÅ›li pytanie zawiera wiele aspektÃ³w, zwrÃ³Ä‡ je osobno, np.:
[author: "mickiewicz"], [subject: "przyroda"], [year_range: "1830-1840"], [place: "warszawa"], [publisher: "Wydawnictwo Naukowe PWN"], [language: "polski"], [document_type: "ksiÄ…Å¼ka"]

Nie powtarzaj pytania uÅ¼ytkownika, nie dodawaj komentarzy, nie zmieniaj nazwisk, tytuÅ‚Ã³w, lat, miejsc wydania, wydawcy, jÄ™zyka ani typu dokumentu.
Nie wpisuj kluczy, ktÃ³rych nie ma w pytaniu.

Dodatkowe zasady interpretacji dat:
1. JeÅ›li w pytaniu jest â€ostatnich X latâ€, przyjmij, Å¼e obecny rok to {current_year}, wiÄ™c zapisz [year_range: "{current_year} - X + 1-{current_year}"].
2. JeÅ›li w pytaniu jest â€przed rokiem YYYYâ€, przyjmij [year_range: "0000-(YYYY-1)"].
3. JeÅ›li w pytaniu jest â€po roku YYYYâ€, przyjmij [year_range: "(YYYY+1)-9999"].
4. JeÅ›li w pytaniu jest â€w roku YYYYâ€, przyjmij [year: "YYYY"].
5. JeÅ›li w pytaniu jest â€od roku YYYY do roku ZZZZâ€, przyjmij [year_range: "YYYY-ZZZZ"].
6. JeÅ›li nie da siÄ™ jednoznacznie ustaliÄ‡, nie wpisuj year ani year_range.

PrzykÅ‚ady:

1. Pytanie uÅ¼ytkownika: "Czy znajdÄ™ ksiÄ…Å¼ki Mickiewicza o przyrodzie z lat 1830-1840 wydane w Warszawie?"
OdpowiedÅº:
[author: "mickiewicz"], [subject: "przyroda"], [year_range: "1830-1840"], [place: "warszawa"]

2. Pytanie uÅ¼ytkownika: "Szukam czasopism o astronomii wydanych w Krakowie."
OdpowiedÅº:
[subject: "astronomia"], [place: "KrakÃ³w"], [document_type: "czasopismo"]

3. Pytanie uÅ¼ytkownika: "Czy macie artykuÅ‚y naukowe o sztucznej inteligencji z lat 2010-2020?"
OdpowiedÅº:
[subject: "sztuczna inteligencja"], [year_range: "2010-2020"], [document_type: "artykuÅ‚ naukowy"]

4. Pytanie uÅ¼ytkownika: "Gdzie mogÄ™ znaleÅºÄ‡ ebooki Stephena Kinga w jÄ™zyku angielskim?"
OdpowiedÅº:
[author: "Stephen King"], [language: "angielski"], [document_type: "ebook"]

5. Pytanie uÅ¼ytkownika: "Czy macie raporty o zmianach klimatycznych wydane przez ONZ?"
OdpowiedÅº:
[subject: "zmiany klimatyczne"], [publisher: "ONZ"], [document_type: "raport"]

6. Pytanie uÅ¼ytkownika: "Szukam audiobookÃ³w z powieÅ›ciami kryminalnymi."
OdpowiedÅº:
[subject: "powieÅ›ci kryminalne"], [document_type: "audiobook"]

7. Pytanie uÅ¼ytkownika: "Czy macie raporty ekonomiczne wydane przez Bank Åšwiatowy oraz Forum Ekonomiczne?"
OdpowiedÅº:
[subject: "ekonomia"], [publisher: "Bank Åšwiatowy"], [publisher: "Forum Ekonomiczne"], [document_type: "raport"]

8. Pytanie uÅ¼ytkownika: "Szukam ksiÄ…Å¼ek o historii Polski lub historii Europy wydanych w latach 2000-2010."
OdpowiedÅº:
[subject: "historia Polski"], [subject: "historia Europy"], [year_range: "2000-2010"], [document_type: "ksiÄ…Å¼ka"]

9. Pytanie uÅ¼ytkownika: "Czy macie artykuÅ‚y naukowe o medycynie oraz raporty o zdrowiu publicznym?"
OdpowiedÅº:
[subject: "medycyna"], [document_type: "artykuÅ‚ naukowy"], [subject: "zdrowie publiczne"], [document_type: "raport"]

10. Pytanie uÅ¼ytkownika: "Czy macie ksiÄ…Å¼ki Stephena Kinga wydane przed 2000 rokiem?"
OdpowiedÅº:
[author: "Stephen King"], [year_range: "0000-1999"], [document_type: "ksiÄ…Å¼ka"]

11. Pytanie uÅ¼ytkownika: "Czy macie artykuÅ‚y naukowe o psychologii z ostatnich 5 lat?"
OdpowiedÅº:
[subject: "psychologia"], [year_range: "{current_year - 4}-{current_year}"], [document_type: "artykuÅ‚ naukowy"]

Pytanie uÅ¼ytkownika, interesuje Ci tylko ono, nic od siebie nie dodawaj, Å¼adnych pytaÅ„, jeÅ›li uÅ¼ytkownik uÅ¼ywa imienia i nazwiska uÅ¼ywaj ich obu, wszystko zawsze w mianowniku, pamiÄ™taj o zasadach dotyczÄ…cych dat i obecnym roku {current_year}!: {{user_question}}
OdpowiedÅº:
"""


if __name__ == "__main__":
    #user_query = "Czy znajdÄ™ ksiÄ…Å¼ki o grach komputerowych?"
    user_query = "â€Czy macie ksiÄ…Å¼ki napisane przez BolesÅ‚awa Prusa, wydane w Warszawie w roku 1890?"
    user_query ="Szukam ksiÄ…Å¼ek o historii Polski w jÄ™zyku angielskim, wydanych przez wydawnictwo Penguin."
    user_query ="Czy macie czasopisma o astronomii wydane w Krakowie?"
    user_query ="Czy macie artykuÅ‚y naukowe o medycynie oraz raporty o zdrowiu publicznym?"
    user_query ="Czy macie raporty ekonomiczne wydane przez Bank Åšwiatowy oraz Forum Europejskie?"
    user_query ="Czy macie czasopisma o technologii oraz artykuÅ‚y naukowe o sztucznej inteligencji?"
    user_query ="Czy macie ksiÄ…Å¼ki Stephena Kinga wydane przed 2000 rokiem oraz artykuÅ‚y naukowe o psychologii z ostatnich 5 lat?"
    user_query ="Czy znajdÄ™ ebooki w jÄ™zyku angielskim autorstwa J.K. Rowling?"
    interp = interpret_prompt.format(user_question=user_query)

    print(">>> interpret_prompt:")
    print(interp)

    interpretation = ask_llama_pllum(interp, max_new_tokens=150)
  #  interpretation = ask_pllum(interp, max_new_tokens=150)
    print("=== INTERPRETACJA ===")
    print(interpretation)
    
    
#%% FALCON   
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "tiiuae/Falcon3-10B-Instruct"

# Åadujemy tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Åadujemy model w trybie 8-bitowym na GPU (oszczÄ™dnoÅ›Ä‡ VRAM)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_8bit=True,
    trust_remote_code=True
)

# Tworzymy pipeline do generowania tekstu
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
)

def ask_falcon(prompt: str, max_new_tokens=150) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.1,   # MoÅ¼esz dostosowaÄ‡
        top_p=0.9,
        repetition_penalty=1.2
    )
    return output[0]["generated_text"]

# PrzykÅ‚adowe zapytanie do modelu:
prompt = "who is mickiewicz?"
response = ask_falcon(prompt)
print(response)
import datetime

# Calculate the current year
current_year = datetime.datetime.now().year

# Dynamically insert the current year into the prompt:
interpret_prompt = f"""You are a library assistant.

Your task is solely to determine whether the user's question contains:
- a title,
- an author,
- a subject,
- a date range (year range),
- a single year (year),
- a place of publication (place),
- a publisher,
- a language,
- a document type,
or any combination of the above.

Present the result **only** in the following format:
[author: "last name"], [subject: "topic"], [title: "title"], [year: "YYYY"], [year_range: "YYYY-YYYY"], [place: "place of publication"], [publisher: "publisher"], [language: "language"], [document_type: "document type"]

If the question contains multiple aspects, list them separately, for example:
[author: "mickiewicz"], [subject: "nature"], [year_range: "1830-1840"], [place: "Warsaw"], [publisher: "Scientific Publishing House PWN"], [language: "Polish"], [document_type: "book"]

Do not repeat the userâ€™s question, do not add any comments, and do not change names, titles, years, places of publication, publishers, languages, or document types.
Do not include keys that are not explicitly mentioned in the question.

### **Additional date interpretation rules:**
1. If the question mentions "the last X years," assume the current year is {current_year}, and record it as [year_range: "{current_year} - X + 1-{current_year}"].
2. If the question asks for "before year YYYY," record it as [year_range: "0000-(YYYY-1)"].
3. If the question asks for "after year YYYY," record it as [year_range: "(YYYY+1)-9999"].
4. If the question asks for "in the year YYYY," record it as [year: "YYYY"].
5. If the question asks for "from year YYYY to year ZZZZ," record it as [year_range: "YYYY-ZZZZ"].
6. If it is not possible to determine the year or range unambiguously, do not include year or year_range.

### **Examples:**

1. **User question:** "Can I find books by Mickiewicz about nature from 1830-1840 published in Warsaw?"
   **Answer:**
   [author: "mickiewicz"], [subject: "nature"], [year_range: "1830-1840"], [place: "Warsaw"]

2. **User question:** "I'm looking for journals about astronomy published in KrakÃ³w."
   **Answer:**
   [subject: "astronomy"], [place: "KrakÃ³w"], [document_type: "journal"]

3. **User question:** "Do you have scientific articles on artificial intelligence from 2010-2020?"
   **Answer:**
   [subject: "artificial intelligence"], [year_range: "2010-2020"], [document_type: "scientific article"]

4. **User question:** "Where can I find ebooks by Stephen King in English?"
   **Answer:**
   [author: "Stephen King"], [language: "English"], [document_type: "ebook"]

5. **User question:** "Do you have reports on climate change published by the UN?"
   **Answer:**
   [subject: "climate change"], [publisher: "UN"], [document_type: "report"]

6. **User question:** "I'm looking for audiobooks with crime novels."
   **Answer:**
   [subject: "crime novels"], [document_type: "audiobook"]

7. **User question:** "Do you have economic reports published by the World Bank and the Economic Forum?"
   **Answer:**
   [subject: "economy"], [publisher: "World Bank"], [publisher: "Economic Forum"], [document_type: "report"]

8. **User question:** "I'm looking for books on the history of Poland or the history of Europe published between 2000-2010."
   **Answer:**
   [subject: "history of Poland"], [subject: "history of Europe"], [year_range: "2000-2010"], [document_type: "book"]

9. **User question:** "Do you have scientific articles on medicine and reports on public health?"
   **Answer:**
   [subject: "medicine"], [document_type: "scientific article"], [subject: "public health"], [document_type: "report"]

10. **User question:** "Do you have books by Stephen King published before the year 2000?"
    **Answer:**
    [author: "Stephen King"], [year_range: "0000-1999"], [document_type: "book"]

11. **User question:** "Do you have scientific articles on psychology from the last 5 years?"
    **Answer:**
    [subject: "psychology"], [year_range: "{current_year - 4}-{current_year}"], [document_type: "scientific article"]

The userâ€™s question is of primary importance. Do not add anything extra, do not ask any questions. If the user provides both a first and last name, use both. Always use the nominative case. Remember the date interpretation rules and the current year {current_year}!  

User question: {{user_question}}

Answer:
"""

interpret_prompt = """Extract relevant metadata from the user's query.

Format:
[author: "author name"], [subject: "topic"], [title: "title"], [year: "YYYY"], 
[year_range: "YYYY-YYYY"], [place: "publication place"], [publisher: "publisher"], 
[language: "language"], [document_type: "document type"]

User question: {user_question}
Answer:
"""






if __name__ == "__main__":
    user_query = "Do you have books written by BolesÅ‚aw Prus, published in Warsaw in the year 1890?"
    user_query = "I'm looking for books on the history of Poland in English, published by Penguin."
    user_query = "Do you have journals about astronomy published in KrakÃ³w?"
    user_query = "Do you have scientific articles on medicine and reports on public health?"
    user_query = "Do you have economic reports published by the World Bank and the European Forum?"
    user_query = "Do you have journals on technology and scientific articles on artificial intelligence?"
    user_query = "Do you have books by Stephen King published before the year 2000 and scientific articles on psychology from the last 5 years?"
    user_query = "Can I find ebooks in English by J.K. Rowling?"
    interp = interpret_prompt.format(user_question=user_query)

    print(">>> interpret_prompt:")
    print(interp)
    response = ask_falcon(interp)
    
    interp = interpret_prompt.format(user_question=user_query)

    print(">>> interpret_prompt:")
    print(interp)

    interpretation = ask_llama_pllum(interp, max_new_tokens=150)
  #  interpretation = ask_pllum(interp, max_new_tokens=150)
    print("=== INTERPRETACJA ===")
    print(interpretation)
    print(repr(interpretation))
    
    
#%% DEEP seek

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                     # Aktywuj kwantyzacjÄ™ 4-bitowÄ…
    bnb_4bit_compute_dtype="float16",      # Ustawienia obliczeniowe
    bnb_4bit_quant_type="nf4",             # UÅ¼ycie Normalized Float 4 (NF4) dla lepszej jakoÅ›ci
    llm_int8_enable_fp32_cpu_offload=True  # Offload obliczeÅ„ na CPU w razie potrzeby
)

model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    quantization_config=quant_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-14B")

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False  # Nie powtarzaj promptu w generowanej odpowiedzi
)

# ğŸ”¹ Funkcja do zadawania pojedynczych pytaÅ„ (bez przechowywania historii)
def ask_model(prompt: str, max_new_tokens=1000) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.1,   # Niska temperatura dla bardziej przewidywalnych odpowiedzi
        top_p=0.7,
        repetition_penalty=1.2
    )
    
    # Pobranie czystego tekstu odpowiedzi
    response = output[0]["generated_text"].strip()
    
    return response

# PrzykÅ‚adowe pytanie do modelu
prompt = "Jakie sÄ… stolice paÅ„stw Europy?"
response = ask_model(prompt)
print(f"ğŸ¤– Model: {response}")  
import datetime

# Calculate the current year
current_year = datetime.datetime.now().year

# Dynamically insert the current year into the prompt:
interpret_prompt = f"""You are a library assistant.

Your task is solely to determine whether the user's question contains:
- a title,
- an author,
- a subject,
- a date range (year range),
- a single year (year),
- a place of publication (place),
- a publisher,
- a language,
- a document type,
or any combination of the above.

Present the result **only** in the following format:
[author: "last name"], [subject: "topic"], [title: "title"], [year: "YYYY"], [year_range: "YYYY-YYYY"], [place: "place of publication"], [publisher: "publisher"], [language: "language"], [document_type: "document type"]

If the question contains multiple aspects, list them separately, for example:
[author: "mickiewicz"], [subject: "nature"], [year_range: "1830-1840"], [place: "Warsaw"], [publisher: "Scientific Publishing House PWN"], [language: "Polish"], [document_type: "book"]

Do not repeat the userâ€™s question, do not add any comments, and do not change names, titles, years, places of publication, publishers, languages, or document types.
Do not include keys that are not explicitly mentioned in the question.

### **Additional date interpretation rules:**
1. If the question mentions "the last X years," assume the current year is {current_year}, and record it as [year_range: "{current_year} - X + 1-{current_year}"].
2. If the question asks for "before year YYYY," record it as [year_range: "0000-(YYYY-1)"].
3. If the question asks for "after year YYYY," record it as [year_range: "(YYYY+1)-9999"].
4. If the question asks for "in the year YYYY," record it as [year: "YYYY"].
5. If the question asks for "from year YYYY to year ZZZZ," record it as [year_range: "YYYY-ZZZZ"].
6. If it is not possible to determine the year or range unambiguously, do not include year or year_range.

### **Examples:**

1. **User question:** "Can I find books by Mickiewicz about nature from 1830-1840 published in Warsaw?"
   **Answer:**
   [author: "mickiewicz"], [subject: "nature"], [year_range: "1830-1840"], [place: "Warsaw"]

2. **User question:** "I'm looking for journals about astronomy published in KrakÃ³w."
   **Answer:**
   [subject: "astronomy"], [place: "KrakÃ³w"], [document_type: "journal"]

3. **User question:** "Do you have scientific articles on artificial intelligence from 2010-2020?"
   **Answer:**
   [subject: "artificial intelligence"], [year_range: "2010-2020"], [document_type: "scientific article"]

4. **User question:** "Where can I find ebooks by Stephen King in English?"
   **Answer:**
   [author: "Stephen King"], [language: "English"], [document_type: "ebook"]

5. **User question:** "Do you have reports on climate change published by the UN?"
   **Answer:**
   [subject: "climate change"], [publisher: "UN"], [document_type: "report"]

6. **User question:** "I'm looking for audiobooks with crime novels."
   **Answer:**
   [subject: "crime novels"], [document_type: "audiobook"]

7. **User question:** "Do you have economic reports published by the World Bank and the Economic Forum?"
   **Answer:**
   [subject: "economy"], [publisher: "World Bank"], [publisher: "Economic Forum"], [document_type: "report"]

8. **User question:** "I'm looking for books on the history of Poland or the history of Europe published between 2000-2010."
   **Answer:**
   [subject: "history of Poland"], [subject: "history of Europe"], [year_range: "2000-2010"], [document_type: "book"]

9. **User question:** "Do you have scientific articles on medicine and reports on public health?"
   **Answer:**
   [subject: "medicine"], [document_type: "scientific article"], [subject: "public health"], [document_type: "report"]

10. **User question:** "Do you have books by Stephen King published before the year 2000?"
    **Answer:**
    [author: "Stephen King"], [year_range: "0000-1999"], [document_type: "book"]

11. **User question:** "Do you have scientific articles on psychology from the last 5 years?"
    **Answer:**
    [subject: "psychology"], [year_range: "{current_year - 4}-{current_year}"], [document_type: "scientific article"]

The userâ€™s question is of primary importance. Do not add anything extra, do not ask any questions. If the user provides both a first and last name, use both. Always use the nominative case. Remember the date interpretation rules and the current year {current_year}!  

User question: {{user_question}}

Answer:
"""


if __name__ == "__main__":
    user_query = "Do you have books written by BolesÅ‚aw Prus, published in Warsaw in the year 1890?"
    user_query = "I'm looking for books on the history of Poland in English, published by Penguin."
    user_query = "Do you have journals about astronomy published in KrakÃ³w?"
    user_query = "Do you have scientific articles on medicine and reports on public health?"
    user_query = "Do you have economic reports published by the World Bank and the European Forum?"
    user_query = "Do you have journals on technology and scientific articles on artificial intelligence?"
    user_query = "Do you have books by Stephen King published before the year 2000 and scientific articles on psychology from the last 5 years?"
    user_query = "Can I find ebooks in English by J.K. Rowling?"
    interp = interpret_prompt.format(user_question=user_query)

    print(">>> interpret_prompt:")
    print(interp)
    response = ask_model(interp)
    print(response)
    





def chat_with_model():
    print("ğŸ’¬ Witaj! MoÅ¼esz rozpoczÄ…Ä‡ rozmowÄ™ z modelem. (Wpisz 'exit' aby zakoÅ„czyÄ‡)")
    
    conversation_history = []  # Historia rozmowy

    while True:
        user_input = input("Ty: ")
        if user_input.lower() == "exit":
            print("ğŸ‘‹ Do zobaczenia!")
            break

        # Tworzenie promptu na podstawie historii rozmowy
        conversation_history.append(f"User: {user_input}")
        prompt = "\n".join(conversation_history) + "\nModel:"

        # Generowanie odpowiedzi
        response = generator(prompt, max_new_tokens=150, temperature=0.7, top_p=0.9)[0]["generated_text"]
        
        # WyodrÄ™bnienie odpowiedzi modelu
        response = response.replace(prompt, "").strip()
        print(f"ğŸ¤– Model: {response}")

        # Dodanie odpowiedzi modelu do historii
        conversation_history.append(f"Model: {response}")

# Uruchomienie czatu
chat_with_model()
#DEEP SEEK CODER
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ğŸ”¹ Konfiguracja kwantyzacji 4-bitowej
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                     
    bnb_4bit_compute_dtype="float16",      
    bnb_4bit_quant_type="nf4",             
    llm_int8_enable_fp32_cpu_offload=True  
)

# ğŸ”¹ Model do zaÅ‚adowania
model_name = "deepseek-ai/DeepSeek-Coder-V2-Lite-Base"

# ğŸ”¹ Åadowanie modelu z `trust_remote_code=True`
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quant_config,
    device_map="auto",
    trust_remote_code=True  # âœ… Pozwala modelowi na wykonanie niestandardowego kodu
)

# ğŸ”¹ Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

print("âœ… Model zaÅ‚adowany pomyÅ›lnie!")

from transformers import pipeline

# Pipeline do generowania kodu
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False
)

# ğŸ”¹ Funkcja do generowania kodu
def ask_model(prompt: str, max_new_tokens=200) -> str:
    output = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.2,  # Niska temperatura dla precyzyjnych wynikÃ³w
        top_p=0.8,
        repetition_penalty=1.2
    )
    return output[0]["generated_text"]

# ğŸ”¹ Test
response = ask_model("Napisz funkcjÄ™ w Pythonie, ktÃ³ra sortuje listÄ™ liczb.")
print(f"ğŸ“œ Wygenerowany kod:\n{response}")
