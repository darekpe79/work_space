{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aaa53a5b-7446-4425-90b1-ba6b90426e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████| 229/229 [00:00<?, ?B/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.json: 100%|███████████████████████████████████████████████████████████████████| 907k/907k [00:00<00:00, 2.76MB/s]\n",
      "merges.txt: 100%|███████████████████████████████████████████████████████████████████████████| 556k/556k [00:00<?, ?B/s]\n",
      "special_tokens_map.json: 100%|█████████████████████████████████████████████████████████| 129/129 [00:00<00:00, 266kB/s]\n",
      "loading file vocab.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\tokenizer_config.json\n",
      "config.json: 100%|████████████████████████████████████████████████████████████████████████████| 664/664 [00:00<?, ?B/s]\n",
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-large-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-large-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from transformers import HerbertTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer with the Polish model\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained('allegro/herbert-large-cased')\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_and_merge_data(json_file_path, excel_file_path, common_column='Link', selected_columns_list=['Tytuł artykułu', 'Tekst artykułu', \"byt 1\", \"zewnętrzny identyfikator bytu 1\", \"Tytuł spektaklu\"]):\n",
    "    # Wczytanie danych z pliku JSON\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "    df_json = pd.DataFrame(json_data)\n",
    "\n",
    "    # Ograniczenie DataFrame JSON do kolumn 'Link' i 'Tekst artykułu'\n",
    "    df_json = df_json[['Link', 'Tekst artykułu']]\n",
    "\n",
    "    # Konwersja wartości w kolumnie 'Tekst artykułu' na stringi\n",
    "    df_json['Tekst artykułu'] = df_json['Tekst artykułu'].astype(str)\n",
    "\n",
    "    # Wczytanie danych z pliku Excel\n",
    "    df_excel = pd.read_excel(excel_file_path)\n",
    "\n",
    "    # Dodanie kolumny indeksowej do DataFrame'a z Excela\n",
    "    df_excel['original_order'] = df_excel.index\n",
    "\n",
    "    # Połączenie DataFrame'ów\n",
    "    merged_df = pd.merge(df_json, df_excel, on=common_column, how=\"inner\")\n",
    "\n",
    "    # Sortowanie połączonego DataFrame według kolumny 'original_order'\n",
    "    merged_df = merged_df.sort_values(by='original_order')\n",
    "\n",
    "    # Konwersja wartości w kolumnach 'Tytuł artykułu' i 'Tekst artykułu' na stringi w połączonym DataFrame\n",
    "    merged_df['Tytuł artykułu'] = merged_df['Tytuł artykułu'].astype(str)\n",
    "    merged_df['Tekst artykułu'] = merged_df['Tekst artykułu'].astype(str)\n",
    "\n",
    "    # Znalezienie indeksu ostatniego wystąpienia 'zewnętrzny identyfikator bytu 1'\n",
    "    if 'zewnętrzny identyfikator bytu 1' in merged_df.columns:\n",
    "        last_id_index = merged_df[merged_df['zewnętrzny identyfikator bytu 1'].notna()].index[-1]\n",
    "        merged_df = merged_df.loc[:last_id_index]\n",
    "    else:\n",
    "        print(\"Brak kolumny 'zewnętrzny identyfikator bytu 1' w DataFrame.\")\n",
    "\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    # Ograniczenie do wybranych kolumn\n",
    "    if set(selected_columns_list).issubset(merged_df.columns):\n",
    "        selected_columns = merged_df[selected_columns_list]\n",
    "    else:\n",
    "        print(\"Nie wszystkie wybrane kolumny są dostępne w DataFrame.\")\n",
    "        selected_columns = merged_df\n",
    "\n",
    "    return selected_columns\n",
    "\n",
    "\n",
    "json_file_path2 = 'C:/Users/User/Desktop/materiał_do_treningu/drive-download-20240125T115916Z-001/jsony/afisz_teatralny_2022-09-08.json'\n",
    "                \n",
    "excel_file_path2 = 'C:/Users/User/Desktop/materiał_do_treningu/drive-download-20240125T115916Z-001/afisz_teatralny_2022-09-08.xlsx'\n",
    "# ... więcej plików w razie potrzeby\n",
    "\n",
    "# Użycie funkcji\n",
    "\n",
    "df2 = load_and_merge_data(json_file_path2, excel_file_path2)\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "\n",
    "df2['combined_text'] = df2['Tytuł artykułu'] + \" \" + df2['Tekst artykułu']\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "from spacy.tokens import Span\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Funkcja do oznaczania słów z tytułów spektakli w tekście\n",
    "def mark_titles(text, title):\n",
    "    # Escapowanie specjalnych znaków w tytule\n",
    "    title_pattern = re.escape(title) + r\"(?![\\w-])\"  # Aby uniknąć dopasowania w środku słowa, dodajemy negative lookahead\n",
    "    # Oznaczanie tytułu w tekście znacznikami\n",
    "    marked_text = re.sub(title_pattern, r\"[PLAY]\\g<0>[/PLAY]\", text, flags=re.IGNORECASE)\n",
    "    return marked_text\n",
    "\n",
    "df2.dropna(subset=['Tytuł spektaklu'], inplace=True)\n",
    "\n",
    "df2['Tytuł spektaklu'] = df2['Tytuł spektaklu'].fillna('')\n",
    "df2['marked_text'] = df2.apply(lambda row: mark_titles(row['combined_text'], row['Tytuł spektaklu']), axis=1)\n",
    "def prepare_data_for_ner(text):\n",
    "    pattern = r\"\\[PLAY\\](.*?)\\[/PLAY\\]\"\n",
    "    entities = []\n",
    "    current_pos = 0\n",
    "    clean_text = \"\"\n",
    "    last_end = 0\n",
    "\n",
    "    for match in re.finditer(pattern, text):\n",
    "        start, end = match.span()\n",
    "        clean_text += text[last_end:start]  # Dodaj tekst przed znacznikiem\n",
    "        start_entity = len(clean_text)\n",
    "        entity_text = match.group(1)\n",
    "        clean_text += entity_text  # Dodaj tekst encji bez znaczników\n",
    "        end_entity = len(clean_text)\n",
    "        entities.append((start_entity, end_entity, \"PLAY\"))\n",
    "        last_end = end  # Zaktualizuj pozycję ostatniego znalezionego końca znacznika\n",
    "\n",
    "    clean_text += text[last_end:]  # Dodaj pozostały tekst po ostatnim znaczniku\n",
    "\n",
    "    return clean_text, {\"entities\": entities}\n",
    "\n",
    "df2['spacy_marked'] = df2['marked_text'].apply(prepare_data_for_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02106ce8-4c31-40c0-8399-17fba2f2f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "transformed_data=df2['spacy_marked'].to_list()\n",
    "def find_nearest_acceptable_split_point(pos, text, total_length):\n",
    "    \"\"\"\n",
    "    Finds the nearest split point that does not split words or sentences,\n",
    "    preferring to split at punctuation followed by space or at natural sentence boundaries.\n",
    "    \"\"\"\n",
    "    if pos <= 0:\n",
    "        return 0\n",
    "    if pos >= total_length:\n",
    "        return total_length\n",
    "    \n",
    "    for offset in range(1, min(50, pos, total_length - pos) + 1):\n",
    "        left = pos - offset\n",
    "        right = pos + offset\n",
    "\n",
    "        if left > 0 and text[left - 1] in '.?!' and text[left] == ' ':\n",
    "            return left + 1\n",
    "\n",
    "        if right < total_length and text[right - 1] in '.?!' and text[right] == ' ':\n",
    "            return right + 1\n",
    "    \n",
    "    for offset in range(1, min(50, pos, total_length - pos) + 1):\n",
    "        left = pos - offset\n",
    "        right = pos + offset\n",
    "\n",
    "        if left > 0 and text[left] == ' ':\n",
    "            return left + 1\n",
    "\n",
    "        if right < total_length and text[right] == ' ':\n",
    "            return right + 1\n",
    "\n",
    "    return pos\n",
    "\n",
    "def split_text_around_entities_adjusted_for_four_parts(data_list):\n",
    "    split_data = []\n",
    "    \n",
    "    for text, annotation in data_list:\n",
    "        entities = sorted(annotation['entities'], key=lambda e: e[0])\n",
    "        total_length = len(text)\n",
    "        ideal_part_length = total_length // 5  # Adjusted for four parts\n",
    "        \n",
    "        split_points = [0]\n",
    "        current_split = 0\n",
    "        \n",
    "        for _ in range(4):  # Adjusted to perform three splits for four parts\n",
    "            proposed_split = current_split + ideal_part_length\n",
    "            if proposed_split >= total_length:\n",
    "                break\n",
    "            \n",
    "            adjusted_split = find_nearest_acceptable_split_point(proposed_split, text, total_length)\n",
    "            \n",
    "            for start, end, _ in entities:\n",
    "                if adjusted_split > start and adjusted_split < end:\n",
    "                    adjusted_split = end\n",
    "                    break\n",
    "            \n",
    "            if adjusted_split != current_split:\n",
    "                split_points.append(adjusted_split)\n",
    "                current_split = adjusted_split\n",
    "        \n",
    "        split_points.append(total_length)\n",
    "        \n",
    "        last_split = 0\n",
    "        for split in split_points[1:]:\n",
    "            part_text = text[last_split:split].strip()\n",
    "            part_entities = [(start - last_split, end - last_split, label) for start, end, label in entities if start >= last_split and end <= split]\n",
    "            split_data.append((part_text, {'entities': part_entities}))\n",
    "            last_split = split\n",
    "\n",
    "    return split_data\n",
    "transformed_data = split_text_around_entities_adjusted_for_four_parts(transformed_data)\n",
    "\n",
    "transformed_data = [data for data in transformed_data if data[1]['entities']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_data_train, transformed_data_eval = train_test_split(transformed_data, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f70b9207-8847-452e-b97e-1fdc4d0232f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([801, 512]) torch.Size([801, 512]) torch.Size([801, 512])\n",
      "Tokens:\n",
      "['<s>', 'Tur', 'nus</w>', 'mija</w>', ',</w>', 'a</w>', 'ja</w>', 'niczy', 'ja</w>', ',</w>', 'reż</w>', '.</w>', 'Cezary</w>', 'Tomaszewski</w>', 'Sam</w>', 'tytuł</w>', 'tego</w>', 'spektaklu</w>', 'wydaje</w>', 'się</w>', 'nieco</w>', 'podejrzany</w>', ',</w>', 'bo</w>', 'cóż</w>', 'możemy</w>', 'sobie</w>', 'pomyśleć</w>', 'kiedy</w>', 'słyszymy</w>', ':</w>', 'Tur', 'nus</w>', 'mija</w>', ',</w>', 'a</w>', 'ja</w>', 'niczy', 'ja</w>', '.</w>', 'Okazuje</w>', 'się</w>', 'jednak</w>', ',</w>', 'że</w>', 'to</w>', 'o</w>', 'czym</w>', 'pomyśle', 'liśmy</w>', 'chociaż</w>', 'przez</w>', 'krótką</w>', 'chwilę</w>', 'jest</w>', 'bardzo</w>', 'złu', 'dne</w>', ',</w>', 'a</w>', 'Cezary</w>', 'Tomaszewski</w>', 'na</w>', 'scenie</w>', 'MOS</w>', 'Teatru</w>', 'Słowackiego</w>', 'w</w>', 'Krakowie</w>', 'tworzy</w>', 'dzieło</w>', 'zaskakujące</w>', 'i</w>', 'nietypowe</w>', '.</w>', 'Zresztą</w>', ',</w>', 'co</w>', 'by</w>', 'nie</w>', 'mówić</w>', 'jest</w>', 'to</w>', 'przecież</w>', 'opere', 'tka</w>', 'sana', 'tory', 'jna</w>', '/</w>', 'fot</w>', '.</w>', 'Monika</w>', 'Stolar', 'ska</w>', '/</w>', 'Na</w>', 'sam</w>', 'dźwięk</w>', 'słowa</w>', 'sanatorium</w>', 'włosy</w>', 'stają</w>', 'dę', 'ba</w>', ',</w>', 'a</w>', 'przed</w>', 'oczami</w>', 'pojawiają</w>', 'się</w>', 'mro', 'czki</w>', '.</w>', 'Coś</w>', 'wewnętrznie</w>', 'zaczyna</w>', 'się</w>', 'dziać</w>', ',</w>', 'a</w>', 'my</w>', 'sami</w>', 'wole', 'libyśmy</w>', 'uciec</w>', 'niż</w>', 'podjąć</w>', 'dyskusję</w>', 'na</w>', 'temat</w>', 'wszelkiej</w>', 'maści</w>', 'uzdrowi', 'sk</w>', '.</w>', 'Ale</w>', 'na</w>', 'naszej</w>', 'drodze</w>', 'pojawia</w>', 'się</w>', 'Cezary</w>', 'Tomaszewski</w>', ',</w>', 'który</w>', 'tę</w>', 'drogę</w>', 'ucieczki</w>', 'odci', 'na</w>', ',</w>', 'a</w>', 'następnie</w>', 'sa', 'dza</w>', 'na</w>', 'teatralnej</w>', 'widowni</w>', 'i</w>', '„</w>', 'zarządza</w>', '”</w>', 'grupowe</w>', 'oglądanie</w>', 'sana', 'tory', 'jnej</w>', 'opere', 'tki</w>', '.</w>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "Input IDs:\n",
      "tensor([    0,  5238, 21502, 17502,  1947,  1011,  2307, 41433,  2307,  1947,\n",
      "        23164,  1899, 19647, 26769,  8895,  6484,  2210, 16774,  5634,  2022,\n",
      "         5477, 26463,  1947,  2338, 15295,  4887,  2669, 19275,  3016, 28449,\n",
      "         1335,  5238, 21502, 17502,  1947,  1011,  2307, 41433,  2307,  1899,\n",
      "        13187,  2022,  2401,  1947,  2040,  2063,  1007,  3572, 15562,  2412,\n",
      "         6103,  2188, 25222,  6571,  2092,  2450, 16786, 26174,  1947,  1011,\n",
      "        19647, 26769,  1998,  9046, 25602, 11277, 16759,  1019,  3992, 12140,\n",
      "        12373, 40232,  1009, 43551,  1899,  9372,  1947,  2249,  2133,  1997,\n",
      "         5310,  2092,  2063,  3886, 44290,  3526, 22416,  6021, 14564,  1888,\n",
      "         8953,  1899,  8408, 24736,  2209,  1888,  2359,  3591, 20847,  5294,\n",
      "        36960, 14980,  8943,  8931,  2347,  1947,  1011,  2534, 20982, 11763,\n",
      "         2022, 13959,  4107,  1899, 15658, 49129,  7894,  2022, 43511,  1947,\n",
      "         1011,  2084,  4062, 27203,  7560, 22216,  2876, 10342, 11527,  1998,\n",
      "         3814, 36459, 49145, 14245,  3044,  1899,  2894,  1998,  4310,  5023,\n",
      "        10331,  2022, 19647, 26769,  1947,  2377,  2769,  7252, 22092,  5300,\n",
      "         1998,  1947,  1011,  5822,  2096,  3568,  1998, 30751, 22257,  1009,\n",
      "         1791, 29367,  1956, 38230, 46337, 22416,  6021, 10932, 44290,  3015,\n",
      "         1899,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "\n",
      "Attention Masks:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Tag IDs:\n",
      "tensor([0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "Tags:\n",
      "['O', 'B-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'I-PLAY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allegro/herbert-large-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"HerbertTokenizerFast\",\n",
      "  \"transformers_version\": \"4.37.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████████████████| 1.63G/1.63G [00:15<00:00, 105MB/s]\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--allegro--herbert-large-cased\\snapshots\\8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at allegro/herbert-large-cased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.sso.sso_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20, Loss: 0.1342208417132497\n",
      "Step 40, Loss: 0.020444209640845656\n",
      "Step 60, Loss: 0.012293809873517603\n",
      "Step 80, Loss: 0.00977769159944728\n",
      "Step 100, Loss: 0.007379752048291266\n",
      "Step 120, Loss: 0.010737980855628848\n",
      "Step 140, Loss: 0.007276899932185188\n",
      "Step 160, Loss: 0.00666877310432028\n",
      "Step 180, Loss: 0.0071296548296231775\n",
      "Step 200, Loss: 0.003851135104196146\n",
      "Step 220, Loss: 0.003618751776230056\n",
      "Step 240, Loss: 0.0036483982432400806\n",
      "Step 260, Loss: 0.0027582176997384524\n",
      "Step 280, Loss: 0.003447389614666463\n",
      "Step 300, Loss: 0.00330568639037665\n",
      "Step 320, Loss: 0.0015641770925867605\n",
      "Step 340, Loss: 0.002058845345163718\n",
      "Step 360, Loss: 0.004316841772924818\n",
      "Step 380, Loss: 0.002364063647837611\n",
      "Step 400, Loss: 0.004792735816954519\n",
      "Step 420, Loss: 0.0027351740602171047\n",
      "Step 440, Loss: 0.0075812273731571626\n",
      "Step 460, Loss: 0.0024735816819884348\n",
      "Step 480, Loss: 0.0021665365558874326\n",
      "Step 500, Loss: 0.00211325043492252\n",
      "Step 520, Loss: 0.0007792154743583524\n",
      "Step 540, Loss: 0.0005702431460576918\n",
      "Step 560, Loss: 0.002875169784510945\n",
      "Step 580, Loss: 0.001989066486203228\n",
      "Step 600, Loss: 0.0018615557481098221\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Przygotowanie tokenizera\n",
    "#tokenizer = HerbertTokenizerFast.from_pretrained('allegro/herbert-base-cased')\n",
    "\n",
    "def prepare_data(data, tokenizer, tag2id, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    \n",
    "    for text, annotation in data:\n",
    "        tokenized_input = tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        offset_mapping = tokenized_input[\"offset_mapping\"].squeeze().tolist()[1:-1]  # Usunięcie mapowania dla [CLS] i [SEP]\n",
    "        sequence_labels = ['O'] * len(offset_mapping)  # Inicjalizacja etykiet jako 'O' dla tokenów (bez [CLS] i [SEP])\n",
    "        \n",
    "        for start, end, label in annotation['entities']:\n",
    "            entity_start_index = None\n",
    "            entity_end_index = None\n",
    "            \n",
    "            for idx, (offset_start, offset_end) in enumerate(offset_mapping):\n",
    "                if start == offset_start or (start > offset_start and start < offset_end):\n",
    "                    entity_start_index = idx\n",
    "                if (end > offset_start and end <= offset_end):\n",
    "                    entity_end_index = idx\n",
    "                    break\n",
    "\n",
    "            if entity_start_index is not None and entity_end_index is not None:\n",
    "                sequence_labels[entity_start_index] = f'B-{label}'  # Begin label\n",
    "                for i in range(entity_start_index + 1, entity_end_index + 1):\n",
    "                    sequence_labels[i] = f'I-{label}'  # Inside label\n",
    "        \n",
    "        # Dodajemy 'O' dla [CLS] i [SEP] oraz dopasowujemy długość etykiet do max_length\n",
    "        full_sequence_labels = ['O'] + sequence_labels + ['O'] * (max_length - len(sequence_labels) - 1)\n",
    "        label_ids = [tag2id.get(label, tag2id['O']) for label in full_sequence_labels]\n",
    "        \n",
    "        input_ids.append(tokenized_input['input_ids'].squeeze().tolist())\n",
    "        attention_masks.append(tokenized_input['attention_mask'].squeeze().tolist())\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "\n",
    "# Mapowanie etykiet\n",
    "tag2id = {'O': 0, 'B-PLAY': 1, 'I-PLAY': 2}\n",
    "\n",
    "# Przygotowanie danych\n",
    "input_ids, attention_masks, labels = prepare_data(transformed_data_train, tokenizer, tag2id)\n",
    "# Przygotowanie danych ewaluacyjnych\n",
    "input_ids_eval, attention_masks_eval, labels_eval = prepare_data(transformed_data_eval, tokenizer, tag2id)\n",
    "\n",
    "\n",
    "# Weryfikacja wyników\n",
    "print(input_ids.shape, attention_masks.shape, labels.shape)\n",
    "\n",
    "example_idx = 0  # indeks przykładu, który chcemy wydrukować\n",
    "\n",
    "# Konwersja input_ids do tokenów\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[example_idx])\n",
    "\n",
    "print(f\"Tokens:\\n{tokens}\\n\")\n",
    "print(f\"Input IDs:\\n{input_ids[example_idx]}\\n\")\n",
    "print(f\"Attention Masks:\\n{attention_masks[example_idx]}\\n\")\n",
    "print(f\"Tag IDs:\\n{labels[example_idx]}\\n\")\n",
    "\n",
    "# Wydrukuj skojarzone z tokenami etykiety (dla lepszej czytelności)\n",
    "tags = [list(tag2id.keys())[list(tag2id.values()).index(tag_id)] if tag_id in tag2id.values() else 'PAD' for tag_id in labels[example_idx]]\n",
    "print(f\"Tags:\\n{tags}\\n\")\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'allegro/herbert-large-cased',\n",
    "    num_labels=len(tag2id)\n",
    ")\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "eval_dataset = TensorDataset(input_ids_eval, attention_masks_eval, labels_eval)\n",
    "\n",
    "# DataLoader dla danych ewaluacyjnych\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=4,  # Dostosuj zgodnie z potrzebami\n",
    "    shuffle=False  # Nie ma potrzeby mieszać danych ewaluacyjnych\n",
    ")\n",
    "# Przygotowanie TensorDataset\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,  # Możesz dostosować w zależności od zasobów\n",
    "    sampler=RandomSampler(train_dataset)  # Mieszanie danych\n",
    ")\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Przenieś model na odpowiednie urządzenie (GPU, jeśli dostępne)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Pętla treningowa\n",
    "from transformers import logging\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "print_loss_every = 20  # Drukuj loss co 50 kroków\n",
    "step = 0\n",
    "\n",
    "for epoch in range(3):  # Liczba epok\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step + 1) % print_loss_every == 0:\n",
    "            print(f\"Step {step + 1}, Loss: {total_loss / print_loss_every}\")\n",
    "            total_loss = 0\n",
    "        \n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b13c936e-a383-45e8-aeec-45460e618843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in C:/Users/User/Desktop/model_NER\\config.json\n",
      "Model weights saved in C:/Users/User/Desktop/model_NER\\model.safetensors\n",
      "tokenizer config file saved in C:/Users/User/Desktop/model_NER\\tokenizer_config.json\n",
      "Special tokens file saved in C:/Users/User/Desktop/model_NER\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"C:/Users/User/Desktop/model_NER\"\n",
    "\n",
    "# Zapisanie modelu\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Zapisanie tokennizera\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "import json\n",
    "\n",
    "# Ścieżka, gdzie chcesz zapisać mapowanie tag2id\n",
    "tag2id_path = \"C:/Users/User/Desktop/model_NER/tag2id.json\"\n",
    "\n",
    "# Zapisanie tag2id do pliku JSON\n",
    "with open(tag2id_path, 'w') as f:\n",
    "    json.dump(tag2id, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c01dff2-6caf-401e-853d-6bc7766c6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.006577966731287154\n",
      "Accuracy: 0.9981346558988764\n",
      "Precision: 0.9981433531953456\n",
      "Recall: 0.9981346558988764\n",
      "F1: 0.9981299433735559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Ustawienie modelu w tryb ewaluacji\n",
    "model.eval()\n",
    "\n",
    "eval_loss = 0\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Iteracja przez DataLoader danych ewaluacyjnych\n",
    "for batch in eval_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_attention_mask, b_labels = batch\n",
    "    \n",
    "    # Wyłączenie obliczania gradientów\n",
    "    with torch.no_grad():\n",
    "        # Przewidywanie etykiet przez model\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, labels=b_labels)\n",
    "    \n",
    "    # Akumulacja straty\n",
    "    eval_loss += outputs.loss.item()\n",
    "    \n",
    "    # Przechwytywanie przewidywań i prawdziwych etykiet\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "# Obliczenie średniej straty\n",
    "eval_loss = eval_loss / len(eval_loader)\n",
    "print(f\"Evaluation loss: {eval_loss}\")\n",
    "\n",
    "# Obliczenie metryk, np. dokładności, precyzji, pełności i miary F1\n",
    "predictions = np.argmax(np.concatenate(predictions, axis=0), axis=2)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Dla uproszczenia: obliczanie dokładności\n",
    "accuracy = accuracy_score(true_labels.flatten(), predictions.flatten())\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Dla bardziej zaawansowanych metryk, możesz dostosować poniższe linijki\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels.flatten(), predictions.flatten(), average='weighted')\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f6c49ab-efc6-43b3-b1dd-119f698ae3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: O\n",
      "23</w>: O\n",
      "października</w>: O\n",
      "klasa</w>: O\n",
      "7</w>: O\n",
      "wybrała</w>: O\n",
      "się</w>: O\n",
      "do</w>: O\n",
      "Teatru</w>: O\n",
      "Cracovia</w>: O\n",
      "na</w>: O\n",
      "spektakl</w>: O\n",
      "„</w>: O\n",
      "Bal: B-PLAY\n",
      "lady: I-PLAY\n",
      "na</w>: I-PLAY\n",
      "”</w>: O\n",
      ".</w>: O\n",
      "Czy</w>: O\n",
      "warto</w>: O\n",
      "obejrzeć</w>: O\n",
      "tę</w>: O\n",
      "sztukę</w>: O\n",
      "?</w>: O\n",
      "Prze: O\n",
      "czyta: O\n",
      "jcie</w>: O\n",
      "opinie</w>: O\n",
      "siód: O\n",
      "mo: O\n",
      "klasistów</w>: O\n",
      ":</w>: O\n",
      "Spektakl</w>: O\n",
      "Bal: B-PLAY\n",
      "lady: I-PLAY\n",
      "na</w>: I-PLAY\n",
      "Dnia</w>: O\n",
      "23</w>: O\n",
      "października</w>: O\n",
      "20: O\n",
      "19</w>: O\n",
      "roku</w>: O\n",
      "w</w>: O\n",
      "Teatrze</w>: O\n",
      "Cracovia</w>: O\n",
      "na</w>: O\n",
      "terenie</w>: O\n",
      "Centrum</w>: O\n",
      "Kultury</w>: O\n",
      "Sol: O\n",
      "va: O\n",
      "y</w>: O\n",
      "odbył</w>: O\n",
      "się</w>: O\n",
      "spektakl</w>: O\n",
      "pod</w>: O\n",
      "tytułem</w>: O\n",
      "Bal: B-PLAY\n",
      "lady: I-PLAY\n",
      "na</w>: I-PLAY\n",
      "na</w>: O\n",
      "podstawie</w>: O\n",
      "dramatu</w>: O\n",
      "Juliusz</w>: O\n",
      "Słowackiego</w>: O\n",
      "o</w>: O\n",
      "takim</w>: O\n",
      "samym</w>: O\n",
      "tytule</w>: O\n",
      ".</w>: O\n",
      "Jest</w>: O\n",
      "on</w>: O\n",
      "prezentowany</w>: O\n",
      "od</w>: O\n",
      "2008</w>: O\n",
      "roku</w>: O\n",
      ".</w>: O\n",
      "Ma</w>: O\n",
      "obecną</w>: O\n",
      "formę</w>: O\n",
      "dzięki</w>: O\n",
      "reżyserii</w>: O\n",
      "i</w>: O\n",
      "scen: O\n",
      "ografii</w>: O\n",
      "Annę</w>: O\n",
      "Kasprzyk</w>: O\n",
      ".</w>: O\n",
      "</s>: O\n"
     ]
    }
   ],
   "source": [
    "def predict_ner(text, model, tokenizer, tag2id):\n",
    "    # Tokenizacja tekstu\n",
    "    tokenized_sentence = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Predykcja modelu\n",
    "    model.eval()  # Ustawienie modelu w tryb ewaluacji\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_sentence)\n",
    "    \n",
    "    # Dekodowanie etykiet\n",
    "    label_indices = np.argmax(output.logits.to('cpu').numpy(), axis=2)\n",
    "    \n",
    "    # Pobranie tokenów i odpowiadających im etykiet\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices[0]):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(list(tag2id.keys())[list(tag2id.values()).index(label_idx)])\n",
    "            new_tokens.append(token)\n",
    "    \n",
    "    # Wyświetlenie tokenów z przewidzianymi etykietami\n",
    "    for token, label in zip(new_tokens, new_labels):\n",
    "        print(f\"{token}: {label}\")\n",
    "\n",
    "# Przykładowy tekst\n",
    "text = '''23 października klasa 7 wybrała się do Teatru Cracovia na spektakl „Balladyna”. Czy warto obejrzeć tę sztukę? Przeczytajcie opinie siódmoklasistów:\n",
    "\n",
    "Spektakl Balladyna\n",
    "\n",
    "Dnia 23 października 2019 roku w Teatrze Cracovia na terenie Centrum Kultury Solvay odbył się spektakl pod tytułem Balladyna na podstawie dramatu Juliusz Słowackiego o takim samym tytule. Jest on prezentowany od 2008 roku. Ma obecną formę dzięki reżyserii i scenografii Annę Kasprzyk.'''\n",
    "\n",
    "# Użycie funkcji\n",
    "predict_ner(text, model, tokenizer, tag2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5c747-18db-4152-a6cd-fefbc52b8bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
